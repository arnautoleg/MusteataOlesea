---
title: "CI"
author: "Oleg Arnaut"
date: "2023-10-04"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

# Точечные оценки

## Показатели центральной тенденции

## Показатели разброса

## Отношение шансов

## Описательные статистики для категориальных переменных

## Зачем нужна визуализация?

```{r, fig.width=10, fig.height=5}


# Create a 2x2 layout for the plots
par(mfrow=c(2, 2))


# Set the parameters for the normal distribution
mean_normal <- 50
sd_normal <- 10
# Generate random data for the normal distribution
data_normal <- rnorm(10000, mean = mean_normal, sd = sd_normal)

# Plot Normal Distribution
hist(data_normal, breaks = 30, col = 'blue', main = 'Normal Distribution (μ=50, σ=10)', xlab = 'Value', ylab = 'Frequency')


# Set the parameters for the uniform distribution
min_uniform <- 44.5
max_uniform <- 55.5

# Generate random data for the uniform distribution
data_uniform <- runif(10000, min = min_uniform, max = max_uniform)
mean(data_uniform)
var(data_uniform)

# Plot Uniform Distribution
hist(data_uniform, breaks = 30, col = 'green', main = 'Uniform Distribution (μ=50, σ=10)', xlab = 'Value', ylab = 'Frequency')


# Set the parameters for the bimodal distribution
mean1_bimodal <- 20
sd1_bimodal <- 10
mean2_bimodal <- 80
sd2_bimodal <- 10

# Generate random data for the bimodal distribution
data_bimodal <- c(rnorm(5000, mean = mean1_bimodal, sd = sd1_bimodal), rnorm(5000, mean = mean2_bimodal, sd = sd2_bimodal))


# Plot Bimodal Distribution
hist(data_bimodal, breaks = 30, col = 'red', main = 'Bimodal Distribution (μ=50, σ=10)', xlab = 'Value', ylab = 'Frequency')

# Generate data for Exponential Distribution with a similar mean and sd
mean_exp <- 50
sd_exp <- 10
lambda_exp <- 1 / mean_exp

data_exponential <- rexp(10000, rate = lambda_exp)

# Plot Exponential Distribution
hist(data_exponential, breaks = 30, col = 'purple', main = 'Exponential Distribution (μ=50, σ=10)', xlab = 'Value', ylab = 'Frequency')

# Reset the layout
par(mfrow=c(1, 1))


```


# Доверительные интервалы для среднего

## Квантили (напомнить)

```{r}

a <- c(0:100)

cat(quantile(a, 0.025), 
      quantile(a, 0.975))

?quantile


```

## Доверительный интервал для среднего значения

```{r}

sample_size <-  100 # Количество респондентов прошедших терапию
Hg_improve  <-  20  # Истинное изменение уровня Hg
Hg_sd       <-  3   # Разброс для истинного изменения уровня Hg


alpha <- 0.05       # порог
x_lower <- qnorm(alpha/2)   # нижняя граница 
x_upper <- qnorm(1-alpha/2)  # верхняя граница

num_samples <- 10000  #  Сколько раз мы набираем выборку?

missed <- sapply(1:num_samples, function (k) {

  Hg_change <- rnorm(sample_size, Hg_improve, Hg_sd)  # Формируем выборку
  lower_bound <- mean(Hg_change) + x_lower*sd(Hg_change)/sqrt(sample_size)  # По выборке оцениваем нижнюю границу доверительного интервала
  upper_bound <- mean(Hg_change) + x_upper*sd(Hg_change)/sqrt(sample_size)  # По выборке оцениваем верхнюю границу доверительного интервала

  true_or_false <- ifelse((lower_bound<Hg_improve) & (upper_bound>Hg_improve), 'Captured', 'Missed') # фиксируем результат

})

table(missed)

```


## Сравнительная характеристика Asymptotic Method (Z-interval), Exact Method (t-interval), Bootstrap Method в случае нормального распределения

```{r}

# Sample data (replace this with your own dataset)
set.seed(123)
data <- rnorm(100, mean = 20, sd = 3)

# Confidence level
confidence_level <- 0.95

# Sample mean and standard deviation
sample_mean <- mean(data)
sample_sd <- sd(data)

# Sample size
sample_size <- length(data)

# 1. Asymptotic Method (Z-interval)
z_critical <- qnorm(1 - (1 - confidence_level) / 2)
margin_of_error <- z_critical * (sample_sd / sqrt(sample_size))
ci_asymptotic <- c(sample_mean - margin_of_error, sample_mean + margin_of_error)

# 2. Exact Method (t-interval)
t_critical <- qt(1 - (1 - confidence_level) / 2, df = sample_size - 1)
margin_of_error <- t_critical * (sample_sd / sqrt(sample_size))
ci_exact <- c(sample_mean - margin_of_error, sample_mean + margin_of_error)

# 3. Bootstrapping Method
library(boot)

# Define a function to calculate the statistic of interest (e.g., mean)
statistic_function <- function(data, indices) {
  sample_data <- data[indices]
  return(mean(sample_data))
}

# Perform bootstrapping

set.seed(123)  # Set seed for reproducibility
boot_results <- boot(data, statistic_function, R = 1000)  # R is the number of bootstrap samples

# Calculate bootstrap confidence interval
ci_bootstrap <- quantile(boot_results$t, c((1 - confidence_level) / 2, 1 - (1 - confidence_level) / 2))

# Print results
cat("Asymptotic Method (Z-interval):", ci_asymptotic, "Ширина:", ci_asymptotic[2]-ci_asymptotic[1], "\n")
cat("Exact Method (t-interval):", ci_exact, "Ширина:", ci_exact[2]-ci_exact[1], "\n")
cat("Bootstrap Method:", ci_bootstrap, "Ширина:", ci_bootstrap[2]-ci_bootstrap[1], "\n")


```


## Сравнительная характеристика Asymptotic Method (Z-interval), Exact Method (t-interval), Bootstrap Method в случае нормального распределения (симуляция)

```{r}

sample_size <-  4 # Количество респондентов прошедших терапию
mean  <-  20  # Истинное изменение уровня Hg
sd    <-  3   # Разброс для истинного изменения уровня Hg

# Confidence level
confidence_level <- 0.95

num_samples <- 10000  #  Сколько раз мы набираем выборку?

# 1. Asymptotic Method (Z-interval)

missed <- sapply(1:num_samples, function (k) {

  data <- rnorm(sample_size, mean = mean, sd = sd)  # Формируем выборку
  sample_mean <- mean(data)
  sample_sd <- sd(data)
  
  z_critical <- qnorm(1 - (1 - confidence_level) / 2)
  margin_of_error <- z_critical * (sample_sd / sqrt(sample_size))
  ci_asymptotic <- c(sample_mean - margin_of_error, sample_mean + margin_of_error)
  true_or_false <- ifelse((ci_asymptotic[1]<mean) & (ci_asymptotic[2]>mean), 'Captured', 'Missed') # фиксируем результат

})

print("Asymptotic Method (Z-interval):")  
print(table(missed))
     


# 2. Exact Method (t-interval)



missed <- sapply(1:num_samples, function (k) {

  data <- rnorm(sample_size, mean = mean, sd = sd)  # Формируем выборку
  sample_mean <- mean(data)
  sample_sd <- sd(data)
  
  t_critical <- qt(1 - (1 - confidence_level) / 2, df = sample_size - 1)
  margin_of_error <- t_critical * (sample_sd / sqrt(sample_size))
  ci_exact <- c(sample_mean - margin_of_error, sample_mean + margin_of_error)
  true_or_false <- ifelse((ci_exact[1]<mean) & (ci_exact[2]>mean), 'Captured', 'Missed') # фиксируем результат

})

print("Exact Method (t-interval)")  
print(table(missed))



# 3. Bootstrapping Method
library(boot)

# Define a function to calculate the statistic of interest (e.g., mean)
statistic_function <- function(data, indices) {
  sample_data <- data[indices]
  return(mean(sample_data))
}

# Perform bootstrapping

missed_bootstrapping <- sapply(1:num_samples, function(k) {
  
  data <- rnorm(sample_size, mean = mean, sd = sd)
  # Perform bootstrapping for each sample
  boot_results <- boot(data, statistic_function, R = 1000)  # R is the number of bootstrap samples
  
  # Calculate bootstrap confidence interval
  ci_bootstrap <- quantile(boot_results$t, c((1 - confidence_level) / 2, 1 - (1 - confidence_level) / 2))
  
  true_or_false <- ifelse((ci_bootstrap[1] < mean) & (ci_bootstrap[2] > mean), 'Captured', 'Missed')  # Record whether the true mean is captured
  
  return(true_or_false)
})

print("Bootstrapping Method")
print(table(missed_bootstrapping))


```


## Сравнительная характеристика Asymptotic Method (Z-interval), Exact Method (t-interval), Bootstrap Method в случае экспоненциального распределения


```{r}

# Specify the desired mean and standard deviation
mean <- 10

# Calculate the rate parameter (λ)
lambda <- 1 / mean

# Generate random data from the exponential distribution with the specified λ
data_exponential <- rexp(100, rate = lambda)

# Check the actual mean and standard deviation of the generated data
actual_mean <- mean(data_exponential)
actual_sd <- sd(data_exponential)

# Print the actual mean and standard deviation
cat("Actual Mean:", actual_mean, "\n")
cat("Actual Standard Deviation:", actual_sd, "\n")

hist(data_exponential)


```

```{r}

# Sample data (replace this with your own dataset)
set.seed(123)
mean = 20
n=100
data <-  rexp(n, rate = 1/mean)

# Confidence level
confidence_level <- 0.95

# Sample mean and standard deviation
sample_mean <- mean(data)
sample_sd <- sd(data)

# Sample size
sample_size <- length(data)

# 1. Asymptotic Method (Z-interval)
z_critical <- qnorm(1 - (1 - confidence_level) / 2)
margin_of_error <- z_critical * (sample_sd / sqrt(sample_size))
ci_asymptotic <- c(sample_mean - margin_of_error, sample_mean + margin_of_error)

# 2. Exact Method (t-interval)
t_critical <- qt(1 - (1 - confidence_level) / 2, df = sample_size - 1)
margin_of_error <- t_critical * (sample_sd / sqrt(sample_size))
ci_exact <- c(sample_mean - margin_of_error, sample_mean + margin_of_error)

# 3. Bootstrapping Method
library(boot)

# Define a function to calculate the statistic of interest (e.g., mean)
statistic_function <- function(data, indices) {
  sample_data <- data[indices]
  return(mean(sample_data))
}

# Perform bootstrapping

set.seed(123)  # Set seed for reproducibility
boot_results <- boot(data, statistic_function, R = 1000)  # R is the number of bootstrap samples

# Calculate bootstrap confidence interval
ci_bootstrap <- quantile(boot_results$t, c((1 - confidence_level) / 2, 1 - (1 - confidence_level) / 2))

# Print results
cat("Asymptotic Method (Z-interval):", ci_asymptotic, "Ширина:", ci_asymptotic[2]-ci_asymptotic[1], "\n")
cat("Exact Method (t-interval):", ci_exact, "Ширина:", ci_exact[2]-ci_exact[1], "\n")
cat("Bootstrap Method:", ci_bootstrap, "Ширина:", ci_bootstrap[2]-ci_bootstrap[1], "\n")


```


## Сравнительная характеристика Asymptotic Method (Z-interval), Exact Method (t-interval), Bootstrap Method в случае экспоненциального распределения (симуляция)

```{r}

sample_size <-  4 # Количество респондентов прошедших терапию
mean  <-  20  # Истинное изменение уровня Hg
sd    <-  3   # Разброс для истинного изменения уровня Hg

# Confidence level
confidence_level <- 0.95

num_samples <- 10000  #  Сколько раз мы набираем выборку?

# 1. Asymptotic Method (Z-interval)

missed <- sapply(1:num_samples, function (k) {

  data <- rexp(sample_size, rate = 1/mean)  # Формируем выборку
  sample_mean <- mean(data)
  sample_sd <- sd(data)
  
  z_critical <- qnorm(1 - (1 - confidence_level) / 2)
  margin_of_error <- z_critical * (sample_sd / sqrt(sample_size))
  ci_asymptotic <- c(sample_mean - margin_of_error, sample_mean + margin_of_error)
  true_or_false <- ifelse((ci_asymptotic[1]<mean) & (ci_asymptotic[2]>mean), 'Captured', 'Missed') # фиксируем результат

})

print("Asymptotic Method (Z-interval):")  
print(table(missed))
     


# 2. Exact Method (t-interval)



missed <- sapply(1:num_samples, function (k) {

  data <- rexp(sample_size, rate = 1/mean)  # Формируем выборку
  sample_mean <- mean(data)
  sample_sd <- sd(data)
  
  t_critical <- qt(1 - (1 - confidence_level) / 2, df = sample_size - 1)
  margin_of_error <- t_critical * (sample_sd / sqrt(sample_size))
  ci_exact <- c(sample_mean - margin_of_error, sample_mean + margin_of_error)
  true_or_false <- ifelse((ci_exact[1]<mean) & (ci_exact[2]>mean), 'Captured', 'Missed') # фиксируем результат

})

print("Exact Method (t-interval)")  
print(table(missed))



# 3. Bootstrapping Method
library(boot)

# Define a function to calculate the statistic of interest (e.g., mean)
statistic_function <- function(data, indices) {
  sample_data <- data[indices]
  return(mean(sample_data))
}

# Perform bootstrapping

missed_bootstrapping <- sapply(1:num_samples, function(k) {
  
  data <- rexp(sample_size, rate = 1/mean)
  # Perform bootstrapping for each sample
  boot_results <- boot(data, statistic_function, R = 1000)  # R is the number of bootstrap samples
  
  # Calculate bootstrap confidence interval
  ci_bootstrap <- quantile(boot_results$t, c((1 - confidence_level) / 2, 1 - (1 - confidence_level) / 2))
  
  true_or_false <- ifelse((ci_bootstrap[1] < mean) & (ci_bootstrap[2] > mean), 'Captured', 'Missed')  # Record whether the true mean is captured
  
  return(true_or_false)
})

print("Bootstrapping Method")
print(table(missed_bootstrapping))


```


## От чего зависит ширина доверительного интервала?


# Доверительный интервал для разницы средних (зависимые и независимые выборки)

```{r}
#https://www.codecamp.ru/blog/confidence-interval-in-r/




#input sample size, sample mean, and sample standard deviation
n1 <- 30
xbar1 <- 310 
s1 <- 18.5

n2 <- 30
xbar2 <- 300
s2 <- 16.4

#calculate pooled variance
sp = ((n1-1)*s1^2 + (n2-1)*s2^2) / (n1+n2-2)

#calculate margin of error
margin <- qt(0.975,df=n1+n2-1)*sqrt(sp/n1 + sp/n2)

#calculate lower and upper bounds of confidence interval
low <- (xbar1-xbar2) - margin
low



high <- (xbar1-xbar2) + margin
high



```

```{r}

# Install and load the necessary libraries if not already installed
# install.packages("shiny")

library(shiny)
library(dplyr)
library(ggplot2)

# Define the UI for the Shiny app
ui <- fluidPage(
  titlePanel("Confidence Interval for Two Means"),

  sidebarLayout(
    sidebarPanel(
      numericInput("sample1_mean", "Sample 1 Mean:", value = 0),
      numericInput("sample1_sd", "Sample 1 Standard Deviation:", value = 1),
      numericInput("sample1_size", "Sample 1 Size:", value = 30),
      numericInput("sample2_mean", "Sample 2 Mean:", value = 0),
      numericInput("sample2_sd", "Sample 2 Standard Deviation:", value = 1),
      numericInput("sample2_size", "Sample 2 Size:", value = 30),
      sliderInput("confidence_level", "Confidence Level:", min = 0.01, max = 0.99, value = 0.95, step = 0.01),
      actionButton("calculate_button", "Calculate")
    ),

    mainPanel(
      plotOutput("result_plot"),
      verbatimTextOutput("result_text")
    )
  )
)

# Define the server logic for the Shiny app
server <- function(input, output) {
  observeEvent(input$calculate_button, {
    # Generate random data for two samples based on user inputs
    sample1 <- rnorm(input$sample1_size, mean = input$sample1_mean, sd = input$sample1_sd)
    sample2 <- rnorm(input$sample2_size, mean = input$sample2_mean, sd = input$sample2_sd)
    
    # Calculate the difference between means
    mean_diff <- mean(sample1) - mean(sample2)
    
    # Calculate the standard error of the difference
    se_diff <- sqrt((var(sample1) / length(sample1)) + (var(sample2) / length(sample2)))
    
    # Calculate the degrees of freedom
    df <- (var(sample1) / length(sample1) + var(sample2) / length(sample2))^2 / 
      ((1 / (length(sample1) - 1)) * (var(sample1) / length(sample1))^2 / (length(sample1) - 1) + 
         (1 / (length(sample2) - 1)) * (var(sample2) / length(sample2))^2 / (length(sample2) - 1))
    
    # Calculate the t-score based on the confidence level
    t_score <- qt((1 + input$confidence_level) / 2, df)
    
    # Calculate the margin of error
    margin_error <- t_score * se_diff
    
    # Calculate the confidence interval
    lower_ci <- mean_diff - margin_error
    upper_ci <- mean_diff + margin_error
    
    # Create a histogram plot
    plot_data <- data.frame(Sample = c(rep("Sample 1", input$sample1_size), rep("Sample 2", input$sample2_size)),
                            Value = c(sample1, sample2))
    
    result_plot <- ggplot(plot_data, aes(x = Value, fill = Sample)) +
      geom_histogram(binwidth = 0.5, position = "identity", alpha = 0.6) +
      geom_vline(xintercept = mean_diff, linetype = "dashed", color = "red", size = 1) +
      labs(title = "Histogram of Two Samples with Confidence Interval",
           x = "Value", y = "Frequency",
           subtitle = paste("Confidence Interval for Difference of Means:",
                             round(lower_ci, 2), "-", round(upper_ci, 2))) +
      scale_fill_manual(values = c("Sample 1" = "blue", "Sample 2" = "green")) +
      theme_minimal()
    
    # Display the result text
    result_text <- paste("Confidence Interval for Difference of Means:",
                         round(lower_ci, 2), "-", round(upper_ci, 2))
    
    # Update the output
    output$result_plot <- renderPlot({ result_plot })
    output$result_text <- renderText({ result_text })
  })
}

# Run the Shiny app
shinyApp(ui, server)


```



# Доверительный интервал для доли

## Wald Interval  https://towardsdatascience.com/five-confidence-intervals-for-proportions-that-you-should-know-about-7ff5484c024f


```{r}

waldInterval <- function(x, n, conf.level = 0.95){
 p <- x/n
 sd <- sqrt(p*((1-p)/n))
 z <- qnorm(c( (1 - conf.level)/2, 1 - (1-conf.level)/2)) #returns the value of thresholds at which conf.level has to be cut at. for 95% CI, this is -1.96 and +1.96
 ci <- p + z*sd
 return(ci)
 }
#example
waldInterval(x = 20, n =40) #this will return 0.345 and 0.655

```

```{r}

numSamples <- 100 #number of samples to be drawn from population
numTrials <- 1000 #this is the sample size (size of each sample)
probs <- seq(0.001, 0.999, 0.01) #true proportions in prevalence. #for each value in this array, we will construct 95% confidence #intervals
coverage <- as.numeric() #initializing an empty vector to store coverage for each of the probs defined above
for (i in 1:length(probs)) {
 x <- rbinom(n = numSamples, size=numTrials, prob = probs[i]) #taken #n random samples and get the number of successes in each of the n #samples. thus x here will have a length equal to n
 isCovered <- as.numeric() #a boolean vector to denote if the true #population proportion (probs[i]) is covered within the constructed ci
 #since we have n different x here, we will have n different ci for #each of them. 
 for (j in 1:numSamples) {
 ci <- waldInterval(x = x[j], n = numTrials)
 isCovered[j] <- (ci[1] < probs[i]) & (probs[i] < ci[2]) #if the #true proportion (probs[i]) is covered within the constructed CI, #then it returns 1, else 0
 }
 coverage[i] <- mean(isCovered)*100 #captures the coverage for each #of the true proportions. ideally, for a 95% ci, this should be more #or else 95%
}


plot(probs, coverage, type='l', ylim = c(75,100), col='blue', lwd=2, frame.plot = FALSE, yaxt='n', main = 'Coverage of Wald Interval',
 xlab = 'True Proportion (Population Proportion) ', ylab = 'Coverage (%) for 95% CI')
abline(h = 95, lty=3, col='maroon', lwd=2)
axis(side = 2, at=seq(75,100, 5))


```


## Clopper — Pearson Interval (Exact Interval)

```{r}

numSamples <- 100 #number of samples to be drawn from population
numTrials <- 1000 #this is the sample size (size of each sample)
probs <- seq(0.001, 0.999, 0.01) #true proportions in prevalence. #for each value in this array, we will construct 95% confidence #intervals
coverage <- as.numeric() #initializing an empty vector to store coverage for each of the probs defined above
for (i in 1:length(probs)) {
 x <- rbinom(n = numSamples, size=numTrials, prob = probs[i]) #taken #n random samples and get the number of successes in each of the n #samples. thus x here will have a length equal to n
 isCovered <- as.numeric() #a boolean vector to denote if the true #population proportion (probs[i]) is covered within the constructed ci
 #since we have n different x here, we will have n different ci for #each of them. 
 for (j in 1:numSamples) {
 ci <- binom.test(x = x[j], n = numTrials)$conf
 isCovered[j] <- (ci[1] < probs[i]) & (probs[i] < ci[2]) #if the #true proportion (probs[i]) is covered within the constructed CI, #then it returns 1, else 0
 }
 coverage[i] <- mean(isCovered)*100 #captures the coverage for each #of the true proportions. ideally, for a 95% ci, this should be more #or else 95%
}
plot(probs, coverage, type='l', ylim = c(75,100), col='blue', lwd=2, frame.plot = FALSE, yaxt='n', main = 'Coverage of Clopper — Pearson',
 xlab = 'True Proportion (Population Proportion) ', ylab = 'Coverage (%) for 95% CI')
abline(h = 95, lty=3, col='maroon', lwd=2)
axis(side = 2, at=seq(75,100, 5))



```






```{r}

# Создайте данные для двух групп (количество успешных и общее количество в каждой группе)
successes_group_A <- 45
total_group_A <- 100
successes_group_B <- 60
total_group_B <- 100

# Оцените доли успешных событий в каждой группе
prop_success_A <- successes_group_A / total_group_A
prop_success_B <- successes_group_B / total_group_B

# Оцените разницу в долях
prop_diff <- prop_success_A - prop_success_B

# Задайте уровень доверия (например, 95%)
confidence_level <- 0.95

# Вычислите стандартную ошибку разницы долей
se_diff <- sqrt((prop_success_A * (1 - prop_success_A) / total_group_A) + (prop_success_B * (1 - prop_success_B) / total_group_B))

# Вычислите критические значения Z для выбранного уровня доверия
alpha <- 1 - confidence_level
z_critical <- qnorm(1 - alpha / 2)

# Вычислите границы доверительного интервала для разницы долей
margin_of_error <- z_critical * se_diff
conf_interval <- prop_diff + c(-margin_of_error, margin_of_error)

# Выведите результаты
cat("Разница долей:", prop_diff, "\n")
cat(paste("Доверительный интервал (", 100 * confidence_level, "%): [", conf_interval[1], ", ", conf_interval[2], "]\n"))



```







# Доверительный интервал для разницы долей (зависимые и независимые выборки)

```{r}
#https://www.codecamp.ru/blog/confidence-interval-in-r/

#input sample sizes and sample proportions
n1 <- 100
p1 <- .62

n2 <- 100
p2 <- .46

#calculate margin of error
margin <- qnorm(0.975)*sqrt(p1*(1-p1)/n1 + p2*(1-p2)/n2)

#calculate lower and upper bounds of confidence interval
low <- (p1-p2) - margin
low

high <- (p1-p2) + margin
high


#https://rpubs.com/AllaT/conf-ints ДОБАВИТЬ !!!!

```





# Доверительный интервал для дисперсии


# Односторонние доверительные интервалы (исследования неменьшей эффективности или превосходства)

```{r}

sample_size <-  100 # Количество респондентов прошедших терапию
Hg_improve  <-  20  # Истинное изменение уровня Hg
Hg_sd       <-  3   # Разброс для истинного изменения уровня Hg

alpha <- 0.05       # порог
x_lower <- qnorm(alpha)    # нижняя граница 
x_upper <- qnorm(1)  # верхняя граница

num_samples <- 10000  #  Сколько раз мы набираем выборку?

missed <- sapply(1:num_samples, function (k) {
   
  Hg_change <- rnorm(sample_size, Hg_improve, Hg_sd)  # Формируем выборку
  lower_bound <- mean(Hg_change) + x_lower*sd(Hg_change)/sqrt(sample_size)  # По выборке оцениваем нижнюю границу доверительного интервала
  upper_bound <- mean(Hg_change) + x_upper*sd(Hg_change)/sqrt(sample_size)  # По выборке оцениваем верхнюю границу доверительного интервала
  
  true_or_false <- ifelse((lower_bound<Hg_improve) & (upper_bound>Hg_improve), 'Captured', 'Missed') # фиксируем результат

})

table(missed)


```


```{r}

# Load the ggplot2 library if not already loaded
# install.packages("ggplot2")
library(ggplot2)

# Generate random data from a normal distribution
data <- rnorm(1000, mean = 0, sd = 1)

# Create a density plot using ggplot2
ggplot(data.frame(x = data), aes(x = x)) +
  geom_density(fill = "blue", color = "black", alpha = 0.5) +
  labs(title = "Density Plot of Normal Distribution", x = "Value", y = "Density")


```



```{r}

# Install and load the necessary libraries if not already installed
# install.packages("shiny")
# install.packages("ggplot2")
library(shiny)
library(ggplot2)

# Define the UI for the Shiny app
ui <- fluidPage(
  titlePanel("Interactive Normal Distribution Density Plot and CDF with Confidence Intervals"),
  
  sidebarLayout(
    sidebarPanel(
      sliderInput("mean", "Mean:", min = -5, max = 5, value = 0, step = 0.1),
      sliderInput("sd", "Standard Deviation:", min = 0.1, max = 3, value = 1, step = 0.1),
      numericInput("quantile_lower", "Lower Quantile:", min = 0, max = 1, value = 0.025, step = 0.001),
      numericInput("quantile_upper", "Upper Quantile:", min = 0, max = 1, value = 0.975, step = 0.001),
      numericInput("x_value", "Calculate PDF/CDF at X:", min = -10, max = 10, value = 0, step = 0.1),
      textOutput("quantile_difference"),
      textOutput("pdf_value"),
      textOutput("cdf_value")
    ),
    
    mainPanel(
      plotOutput("densityPlot")
    )
  )
)

# Define a function to calculate ideal PDF values
calculateIdealPDF <- function(x, mean, sd) {
  pdf_values <- dnorm(x, mean = mean, sd = sd)
  return(pdf_values)
}

# Define the server logic for the Shiny app
server <- function(input, output) {
  
  # Create a reactive expression for calculating pdf_values
  pdf_values <- reactive({
    # Generate a range of x-values
    x_range <- seq(-10, 10, by = 0.1)
    
    # Calculate ideal PDF values using the function
    pdf_values <- calculateIdealPDF(x_range, input$mean, input$sd)
    return(pdf_values)
  })
  
  
  
  output$densityPlot <- renderPlot({
    # Generate a larger sample of data for smoother plot
    #data <- rnorm(100000, mean = input$mean, sd = input$sd)
    #x_range <- seq(-10, 10, by = 0.1)
    
    pdf_values_data <- pdf_values()
    
    
    #browser()
    # Calculate quantile values
    quant_lower <- quantile(pdf_values_data, input$quantile_lower)
    quant_upper <- quantile(pdf_values_data, input$quantile_upper)
    
    # Create a density plot with customized text size and font
    plot <- ggplot(data.frame(x = seq(-10, 10, by = 0.1), y = pdf_values_data), aes(x = x, y = pdf)) +
      geom_density(fill = "blue", color = "black", alpha = 0.5) +
      geom_vline(xintercept = quant_lower, linetype = "dashed", color = "red") +
      geom_vline(xintercept = quant_upper, linetype = "dashed", color = "green") 
      geom_vline(xintercept = input$x_value, linetype = "dotted", color = "purple") +  # Add vertical line for X
      geom_text(
        aes(x = input$x_value, y = 0.2),
        label = paste("X =", input$x_value),
        hjust = -0.2,
        size = 4
      ) +
      labs(
        title = "Smoothed Density Plot of Normal Distribution",
        x = "Value",
        y = "Density",
        caption = paste(
          "Lower Quantile:", round(quant_lower, 2),
          "\nUpper Quantile:", round(quant_upper, 2)
        )
      ) +
      theme_minimal() +  # You can change the theme if you prefer
      theme(
        text = element_text(size = 16),  # Adjust text size as needed
        plot.title = element_text(size = 20, face = "bold"),  # Title font size and style
        axis.title = element_text(size = 18),  # Axis labels font size
        axis.text = element_text(size = 14)  # Tick labels font size
      )

    # Calculate the PDF value at the specified x value
    pdf_value <- dnorm(input$x_value, mean = input$mean, sd = input$sd)
    
    # Calculate the CDF value at the specified x value
    cdf_value <- pnorm(input$x_value, mean = input$mean, sd = input$sd)
    
    # Print the PDF and CDF values
    print(plot)
    print(paste("PDF at X:", round(pdf_value, 4)))
    print(paste("CDF at X:", round(cdf_value, 4)))
  })
  
  output$quantile_difference <- renderText({
    # Calculate the difference between upper and lower quantiles
    diff_quantiles <- input$quantile_upper - input$quantile_lower
    paste("Quantile Difference:", round(diff_quantiles, 2))
  })
  
  output$pdf_value <- renderText({
    # Calculate the PDF value at the specified x value
    pdf_value <- dnorm(input$x_value, mean = input$mean, sd = input$sd)
    paste("PDF at X:", round(pdf_value, 4))
  })
  
  output$cdf_value <- renderText({
    # Calculate the CDF value at the specified x value
    cdf_value <- pnorm(input$x_value, mean = input$mean, sd = input$sd)
    paste("CDF at X:", round(cdf_value, 4))
  })
}

# Run the Shiny app
shinyApp(ui, server)

```



# Доверительный интервал для OR (отношение шансов)
# https://search.r-project.org/CRAN/refmans/epitools/html/oddsratio.html


```{r}
install.packages('oddsratio')


library(oddsratio)
library(mgcv)
fit_gam <- gam(y ~ s(x0) + s(I(x1^2)) + s(x2) +
offset(x3) + x4, data = data_gam) # fit model
# create input objects (plot + odds ratios)
plot_object <- plot_gam(fit_gam, pred = "x2", title = "Predictor 'x2'")
or_object1 <- or_gam(
data = data_gam, model = fit_gam,
pred = "x2", values = c(0.099, 0.198)
)


```



```{r}

# Define the values from your contingency table
a <- 30  # Number of observations in Group 1 with Outcome 1
b <- 20  # Number of observations in Group 2 with Outcome 1
c <- 10  # Number of observations in Group 1 with Outcome 2
d <- 40  # Number of observations in Group 2 with Outcome 2

# Calculate the odds ratio (OR)
OR <- (a / c) / (b / d)

# Calculate the standard error (SE) of ln(OR)
SE <- sqrt((1 / a) + (1 / b) + (1 / c) + (1 / d))

# Set the confidence level (e.g., 95%)
confidence_level <- 0.95

# Calculate the critical value (Z) for the confidence interval
Z <- qnorm(1 - (1 - confidence_level) / 2)

# Calculate the lower and upper limits of the confidence interval for ln(OR)
LL <- log(OR) - (Z * SE)
UL <- log(OR) + (Z * SE)

# Calculate the confidence interval for OR
OR_CI <- exp(c(LL, UL))

# Print the results
cat("Odds Ratio (OR):", OR, "\n")
cat("Confidence Interval (95%) for OR:", OR_CI[1], "to", OR_CI[2], "\n")


```


```{r}

# Define the values from your contingency table
a <- 30  # Number of observations in Group 1 with Outcome 1
b <- 20  # Number of observations in Group 2 with Outcome 1
c <- 10  # Number of observations in Group 1 with Outcome 2
d <- 40  # Number of observations in Group 2 with Outcome 2

# Create a 2x2 contingency table
contingency_table <- matrix(c(a, b, c, d), nrow = 2)
rownames(contingency_table) <- c("Outcome 1", "Outcome 2")
colnames(contingency_table) <- c("Group 1", "Group 2")

# Perform Fisher's exact test to calculate OR and its exact confidence interval
fisher_result <- fisher.test(contingency_table, alternative = "two.sided", conf.int = TRUE)

# Extract the OR and its exact confidence interval
OR <- fisher_result$estimate
OR_CI <- fisher_result$conf.int

# Print the results
cat("Odds Ratio (OR):", OR, "\n")
cat("Exact Confidence Interval for OR:", OR_CI[1], "to", OR_CI[2], "\n")


```


```{r}

# Load the boot package
library(boot)

# Define your data
a <- 30  # Number of observations in Group 1 with Outcome 1
b <- 20  # Number of observations in Group 2 with Outcome 1
c <- 10  # Number of observations in Group 1 with Outcome 2
d <- 40  # Number of observations in Group 2 with Outcome 2

# Define a function to calculate the OR
calculate_OR <- function(data, indices) {
  sampled_data <- data[indices, ]
  a <- sum(sampled_data$Outcome == "Outcome1" & sampled_data$Group == "Group1")
  b <- sum(sampled_data$Outcome == "Outcome1" & sampled_data$Group == "Group2")
  c <- sum(sampled_data$Outcome == "Outcome2" & sampled_data$Group == "Group1")
  d <- sum(sampled_data$Outcome == "Outcome2" & sampled_data$Group == "Group2")
  
  OR <- (a / c) / (b / d)
  return(OR)
}



# Create a data frame with your data
data <- data.frame(
  Group = c(rep("Group1", a + c), rep("Group2", b + d)),
  Outcome = c(rep("Outcome1", a), rep("Outcome2", c), rep("Outcome1", b), rep("Outcome2", d))
)

# Set the number of bootstrap samples
n_bootstrap_samples <- 1000

# Perform bootstrapping to calculate the CI for OR
boot_results <- boot(data, statistic = calculate_OR, R = n_bootstrap_samples)

# Calculate the CI
OR_CI <- boot.ci(boot_results, type = "bca")

# Print the results
print(OR_CI)


```


```{r}

# Load necessary libraries
library(boot)


# Define a function to calculate asymptotic CI for OR
calculate_asymptotic_CI <- function(data, confidence_level) {
  # Implement the calculation of asymptotic CI for OR
  # Replace the following lines with your actual code
  OR <- calculate_OR(data)  # Calculate OR using your method
  SE <- calculate_SE(data)  # Calculate standard error of ln(OR)
  z <- qnorm(1 - (1 - confidence_level) / 2)
  lower_limit <- log(OR) - z * SE
  upper_limit <- log(OR) + z * SE
  return(c(exp(lower_limit), exp(upper_limit)))  # Return CI as a numeric vector
}

calculate_asymptotic_CI(simulated_data, confidence_level)


# Define a function to calculate exact CI for OR
calculate_exact_CI <- function(data, confidence_level) {
  # Implement the calculation of exact CI for OR
  # Replace the following lines with your actual code
  OR <- calculate_OR(data)  # Calculate OR using your method
  # Add code to perform exact calculation of CI
  return(c(lower_limit, upper_limit))  # Return CI as a numeric vector
}

# Define a function to calculate bootstrap CI for OR
calculate_bootstrap_CI <- function(data, confidence_level) {
  # Implement the calculation of bootstrap CI for OR
  # Replace the following lines with your actual code
  OR <- calculate_OR(data)  # Calculate OR using your method
  # Add code to perform bootstrap calculation of CI
  return(c(lower_limit, upper_limit))  # Return CI as a numeric vector
}

# Rest of the simulation code remains the same




# Define a function to generate simulated data
generate_data <- function(n, p, OR) {
  a <- round(n * p)
  c <- n - a
  b <- round(a / OR)
  d <- c - b
  
  # Create a 2x2 contingency table
  contingency_table <- matrix(c(a, b, c, d), nrow = 2)
  rownames(contingency_table) <- c("Outcome 1", "Outcome 2")
  colnames(contingency_table) <- c("Group 1", "Group 2")
  
  return(contingency_table)
}

# Set simulation parameters
n_simulations <- 1000  # Number of simulations
n_samples <- 100       # Sample size in each simulation
true_OR <- 2           # True odds ratio to simulate
confidence_level <- 0.95

# Initialize storage for results
results <- data.frame(Method = character(n_simulations), 
                       Coverage = numeric(n_simulations))

# Run simulations
for (i in 1:n_simulations) {
  # Generate simulated data
  simulated_data <- generate_data(n_samples, 0.5, true_OR)
  
  # Calculate CIs using different methods
  asymptotic_CI <- calculate_asymptotic_CI(simulated_data, confidence_level)
  exact_CI <- calculate_exact_CI(simulated_data, confidence_level)
  bootstrap_CI <- calculate_bootstrap_CI(simulated_data, confidence_level)
  
  # Determine which method provided the true OR within the CI
  asymptotic_coverage <- (asymptotic_CI[1] <= true_OR) & (true_OR <= asymptotic_CI[2])
  exact_coverage <- (exact_CI[1] <= true_OR) & (true_OR <= exact_CI[2])
  bootstrap_coverage <- (bootstrap_CI[1] <= true_OR) & (true_OR <= bootstrap_CI[2])
  
  # Store results
  if (asymptotic_coverage) {
    results[i, ] <- c("Asymptotic", 1)
  } else if (exact_coverage) {
    results[i, ] <- c("Exact", 1)
  } else if (bootstrap_coverage) {
    results[i, ] <- c("Bootstrap", 1)
  } else {
    results[i, ] <- c("None", 0)
  }
}

# Calculate coverage probabilities
coverage_probabilities <- table(results$Method) / n_simulations

# Print coverage probabilities
print(coverage_probabilities)


```



```{r}

calculate_asymptotic_CI <- function(contingency_table, confidence_level = 0.95) {
  # Calculate the odds ratio (OR) from the contingency table
  a <- contingency_table[1, 1]
  b <- contingency_table[1, 2]
  c <- contingency_table[2, 1]
  d <- contingency_table[2, 2]
  
  OR <- (a * d) / (b * c)
  
  # Calculate the standard error (SE) of the log(OR)
  SE_log_OR <- sqrt((1/a) + (1/b) + (1/c) + (1/d))
  
  # Calculate the z-score for the specified confidence level
  z <- qnorm(1 - (1 - confidence_level) / 2)
  
  # Calculate the lower and upper limits of the asymptotic CI
  lower_limit <- log(OR) - z * SE_log_OR
  upper_limit <- log(OR) + z * SE_log_OR
  
  # Transform the limits back to the OR scale
  asymptotic_CI <- exp(c(lower_limit, upper_limit))
  
  return(asymptotic_CI)
}

# Example usage:
# Create a 2x2 contingency table
contingency_table <- matrix(c(30, 20, 10, 40), nrow = 2)
rownames(contingency_table) <- c("Outcome 1", "Outcome 2")
colnames(contingency_table) <- c("Group 1", "Group 2")

# Calculate asymptotic CI for OR with a 95% confidence level
asymptotic_CI <- calculate_asymptotic_CI(contingency_table)
print(asymptotic_CI)


```




# Проверка статистических гипотез


```{r}
#https://machinelearningmastery.com/statistical-hypothesis-tests-in-python-cheat-sheet/

#https://analyse-it.com/docs/user-guide/101/estimation

#https://analyse-it.com/docs/user-guide/101/exact-asymptotic-p-values#:~:text=A%20p%2Dvalue%20calculated%20using,about%20the%20hypothesis%20of%20interest.


#https://statistics.laerd.com/statistical-guides/mann-whitney-u-test-assumptions.php#:~:text=Assumption%20%234%3A%20You%20must%20determine,shape%20or%20a%20different%20shape. Mann Withney
x <- c(5,  5,  5,  4)
y <- c(0, 0, 0, 0)
wilcox.test(x, y, paired = FALSE, alternative = "greater")





```


# Chi squared with bootstrap

```{r}
#https://search.r-project.org/CRAN/refmans/DCluster/html/achisq.boot.html

install.packages('spdep')
install.packages('DCluster')

library(boot)
library(spdep)
library(DCluster)

data(nc.sids)

sids<-data.frame(Observed=nc.sids$SID74)
sids<-cbind(sids, Expected=nc.sids$BIR74*sum(nc.sids$SID74)/sum(nc.sids$BIR74))

niter<-1000

# Define the achisq.boot statistic function
# achisq.boot <- function(data, indices) {
#   observed <- data$Observed[indices]
#   expected <- data$Expected[indices]
#   chi_square <- sum((observed - expected)^2 / expected)
#   return(chi_square)
# }


#Permutation  model
chq.perboot<-boot(sids, statistic=achisq.boot, R=niter)
plot(chq.perboot)#Display results

#Multinomial model
chq.mboot<-boot(sids, statistic=achisq.pboot, sim="parametric", ran.gen=multinom.sim,  R=niter)
plot(chq.mboot)#Display results

#Poisson model
chq.pboot<-boot(sids, statistic=achisq.pboot, sim="parametric", ran.gen=poisson.sim,  R=niter)
plot(chq.pboot)#Display results

#Poisson-Gamma model
chq.pgboot<-boot(sids, statistic=achisq.pboot, sim="parametric", ran.gen=negbin.sim, R=niter)
plot(chq.pgboot)#Display results


```

```{r}

#https://livebook.manning.com/book/r-in-action/chapter-12/1 - permutation and bootstrepping

#https://www.rdocumentation.org/packages/RDS/versions/0.8-1/topics/bootstrap.contingency.test

# https://www.tandfonline.com/doi/full/10.1080/10543406.2014.920851?scroll=top&needAccess=true


if(!require('RDS')) {
    install.packages('RDS')
    library('RDS')
}

data(faux)

bootstrap.contingency.test(rds.data=faux, row.var="X", col.var="Y",
  number.of.bootstrap.samples=1000, verbose=FALSE)

data(faux)
convergence.plot(faux,c("X","Y"))




```


```{r}

install.packages("boot")



```




```{r}

waldInterval <- function(x, n, conf.level = 0.95){
 p <- x/n
 sd <- sqrt(p*((1-p)/n))
 z <- qnorm(c( (1 - conf.level)/2, 1 - (1-conf.level)/2)) #returns the value of thresholds at which conf.level has to be cut at. for 95% CI, this is -1.96 and +1.96
 ci <- p + z*sd
 return(ci)
 }
#example
waldInterval(x = 20, n =40) #this will return 0.345 and 0.655


numSamples <- 10000 #number of samples to be drawn from population
numTrials <- 100 #this is the sample size (size of each sample)

probs <- seq(0.001, 0.999, 0.01) #true proportions in prevalence. #for each value in this array, we will construct 95% confidence #intervals
coverage <- as.numeric() #initializing an empty vector to store coverage for each of the probs defined above
for (i in 1:length(probs)) {
 x <- rbinom(n = numSamples, size=numTrials, prob = probs[i]) #taken #n random samples and get the number of successes in each of the n #samples. thus x here will have a length equal to n
 isCovered <- as.numeric() #a boolean vector to denote if the true #population proportion (probs[i]) is covered within the constructed ci
 #since we have n different x here, we will have n different ci for #each of them. 
 for (j in 1:numSamples) {
 ci <- binom.test(x = x[j], n = numTrials)$conf
 isCovered[j] <- (ci[1] < probs[i]) & (probs[i] < ci[2]) #if the #true proportion (probs[i]) is covered within the constructed CI, #then it returns 1, else 0
 }
 coverage[i] <- mean(isCovered)*100 #captures the coverage for each #of the true proportions. ideally, for a 95% ci, this should be more #or else 95%
}



# Create the plot
plot(probs, coverage, type="l", ylim = c(75,100), col="blue", lwd=2, frame.plot = FALSE, yaxt='n', main = "Coverage of Wald Interval",
     xlab = "True Proportion (Population Proportion)", ylab = "Coverage (%) for 95% CI")
abline(h = 95, lty=3, col="maroon", lwd=2)
axis(side = 2, at=seq(75,100, 5))
     



```



```{r}



# Define your data
SCALE_1 <- c(3, 5, 5, 4, 4, 3, 4, 2, 2, 3, 2, 1, 1, 2, 0, 2, 5, 4, 5, 4, 0, 0, 0, 0)
SCALE_2 <- c(2, 2, 3, 2, 2, 1, 2, 3, 2, 1, 1, 2, 2, 1, 1, 1, 3, 3, 2, 3, 0, 0, 0, 0)


# Create a data frame with your data
my_data_frame <- data.frame(SCALE_1 = SCALE_1, SCALE_2 = SCALE_2)
print(my_data_frame)

my_data_frame$SCALE_1 <- factor(my_data_frame$SCALE_1, levels = c(0, 1, 2, 3, 4, 5), labels = c("Изменений нет",
                                                                                                "Единичные", 
                                                                                                "Кольцо 1 слой",
                                                                                                "Кольцо 2 слоя",
                                                                                                "Кольцо 3 слоя",
                                                                                                "Кольцо 4 и > слоёв"))

my_data_frame$SCALE_2 <- factor(my_data_frame$SCALE_2, levels = c(0, 1, 2, 3), labels = c("Изменений нет",
                                                                                       "Лёгкая", 
                                                                                       "Умеренная",
                                                                                       "Выраженная"))

# Print the data frame
print(my_data_frame)

df3 <- my_data_frame %>% 
  group_by(SCALE_1, SCALE_2) %>% 
  tally() %>% 
  complete(SCALE_2, fill = list(n = 0)) %>% 
  mutate(percentage = n / sum(n) * 100)


ggplot(df3, aes(SCALE_1, percentage, fill = SCALE_2)) + 
  geom_bar(stat = 'identity', position = 'dodge') +
  ylim(0, 100)+
  ylab("%")+
  scale_fill_brewer(palette = "Set2")+
  theme(legend.position = "right", axis.text.x = element_text(angle = 45, hjust = 1))




# 1. convert the data as a table
dt <- as.matrix(table(my_data_frame$SCALE_1, my_data_frame$SCALE_2))
dt

#dt
# 2. Graph
#balloonplot(t(dt), xlab ="", ylab="",
#            label = FALSE, 
#            repel = TRUE,
#            show.margins = FALSE)

chisq <- chisq.test(dt)
chisq

res.ca <- FactoMineR::CA(dt, graph = FALSE)
# inspect results of the CA
#print(res.ca)
eig.val <- get_eigenvalue(res.ca)
eig.val

# repel= TRUE to avoid text overlapping (slow if many point)
fviz_ca_biplot(res.ca, 
               repel = TRUE,
               col.row = "orange",
               col.col = "darkgray")

```




```{r}

library(ggplot2)

# Set a seed for reproducibility
set.seed(123)

# Generate synthetic data with a Pearson correlation of 0.85
correlation <- 0.85
n <- 100  # Number of data points

# Generate IL1 and TNF variables
IL1 <- rnorm(n)
TNF <- correlation * IL1 + sqrt(1 - correlation^2) * rnorm(n)

# Create a data frame
data <- data.frame(IL1, TNF)

# Calculate the Pearson correlation coefficient
pearson_corr <- cor(data$IL1, data$TNF)

# Create a scatter plot with increased font size
ggplot(data, aes(x = IL1, y = TNF)) +
  geom_point() +
  labs(title = paste("Pearson Correlation =", round(pearson_corr, 2))) +
  theme(
    text = element_text(size = 12),  # Adjust the font size
    axis.text.x = element_text(size = 12),  # Adjust x-axis label font size
    axis.text.y = element_text(size = 12),  # Adjust y-axis label font size
    plot.title = element_text(size = 14)  # Adjust title font size
  )

```



```{r}


# Load the required libraries
library(ggplot2)

# Sample data with a U-shaped relationship
x <- seq(-2, 2, length.out = 100)
y <- x^2 + rnorm(100)
data <- data.frame(X = x, Y = y)

# Create a scatter plot
ggplot(data, aes(x = X, y = Y)) +
  geom_point() +
  labs(x = "X-axis", y = "Y-axis", title = "Scatter Plot of a U-Shaped Relationship")



```



```{r}

# Load the required libraries
library(ggplot2)

# Fictitious preclinical research data with a U-shaped relationship
dose <- seq(-2, 2, length.out = 100)  # Dose levels
response <- x^2 + rnorm(100)  # Response with a U-shaped pattern

data <- data.frame(Dose = dose, Response = response)

# Create a scatter plot
ggplot(data, aes(x = Dose, y = Response)) +
  geom_point() +
  labs(x = "Dose", y = "Response", title = "Scatter Plot of Dose vs. Response (U-Shaped Relationship)")


correlation <- cor(data$Dose, data$Response)


ggplot(data, aes(x = Dose, y = Response)) +
  geom_point() +
  labs(x = "Dose", y = "Response", title = "Scatter Plot of Dose vs. Response (U-Shaped Relationship)") +
  annotate("text", x = 0, y = max(data$Response), 
           label = paste("Pearson Correlation =", round(correlation, 2)), 
           hjust = 0, vjust = 1)

```


# 95% conf interval for  OR 

```{r}

# Install and load the Epi package
if (!requireNamespace("Epi", quietly = TRUE)) {
  install.packages("Epi")
}

library(Epi)

# Create a vector of sample sizes ranging from 10 to 100 with a step of 5
sample_sizes <- seq(10, 100, by = 5)

# Set the number of bootstrap iterations
num_bootstraps <- 1000

# Create an empty data frame to store the results
results_df <- data.frame(SampleSize = numeric(0),
                         OR = numeric(0),
                         CI_exact_lower = numeric(0),
                         CI_exact_upper = numeric(0),
                         CI_bootstrap_lower = numeric(0),
                         CI_bootstrap_upper = numeric(0))

# Loop through each sample size and odds ratio and calculate confidence intervals
for (sample_size in sample_sizes) {
  for (or_value in odds_ratios) {
    # Set the exposure and outcome counts based on the sample size
    exposure_cases <- sample_size * 0.5
    exposure_controls <- sample_size * 0.5
    outcome_cases <- sample_size * 0.25
    outcome_controls <- sample_size * 0.25

    # Create a 2x2 table
    tab <- matrix(c(outcome_cases, outcome_controls, exposure_cases, exposure_controls), nrow = 2)

    # Calculate odds ratio and perform exact test
    or_result <- epi.2by2(tab, conf.level = 0.95, method = "exact")

    # Calculate bootstrap confidence intervals
    boot_results <- boot.epi.2by2(tab, B = num_bootstraps)

    # Append the results to the data frame
    results_df <- rbind(results_df, data.frame(
      SampleSize = sample_size,
      OR = or_value,
      CI_exact_lower = or_result$measure$measure[1],
      CI_exact_upper = or_result$measure$measure[3],
      CI_bootstrap_lower = quantile(boot_results$or, (1 - 0.95) / 2),
      CI_bootstrap_upper = quantile(boot_results$or, 1 - (1 - 0.95) / 2)
    ))
  }
}

# View the results
print(results_df)






```


